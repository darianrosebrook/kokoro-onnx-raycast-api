To continuously improve and to coach each other, we propose a rubric for what "AI literacy" looks like in design work. In project post-mortems or peer reviews, score each aspect from 0 (not evident) to 3 (exemplary):

1. Decomposition: Did the person break down the work into testable, manageable units? Or did they throw a "do everything" prompt at the AI? High scores mean they clearly separated tasks (and likely had multiple prompt interactions with clear goals) rather than one giant, opaque session.

2. Specification: How clear and crisp were the prompts in defining constraints and acceptance criteria? A top score implies the designer avoided vague "make it cooler" prompts and instead provided concrete instructions (files, styles, goals, non-goals). Lower scores if the prompts were primarily "vibe-based" with little guidance.

3. Verification: Did the designer build in checks? For example, after AI output, did they run accessibility checks, tests, or at least eyeball critical metrics? High score if they caught issues (or the AI's mistakes) via deliberate verification steps. Zero if they merged AI output with no validation and it later had obvious flaws.

4. Rationale: Does the person capture the why behind changes? In PR descriptions or documentation, do they note decisions, alternatives considered, and trade-offs? It's a good sign if an AI-suggested change comes with an explanation in the commit message or PR (showing the human thought process stayed in the loop). Aim for rationales that stand on their own, so someone else can understand the changes without guessing.

5. Resilience: If the AI was unavailable or wrong, did the designer demonstrate they could proceed regardless? This might show up in how they handled an outage (did they have a manual plan?) or if they quickly reformulated tasks after AI failures. Also, using Git robustly (e.g., recovering an older version after a bad AI edit) is part of resilience. Full marks if they treated AI as assistive, not crutch – able to continue the project even if AI interventions were delayed or removed.

6. Ethics & Privacy: Did the designer avoid putting sensitive data into prompts (customer data, confidential designs)? Were they mindful of licensing (e.g., not asking AI to produce images in the style of an artist without permission)? Also, did they recognize or mitigate bias in outputs? A strong score means they used AI responsibly – e.g., they used an internal model for sensitive content, they stripped personally identifiable info from prompts, they fact-checked health or legal info, etc. Basically, the designer understood the ethical stakes and navigated them consciously.

7. Impact: Finally, did the use of AI demonstrably improve the work's speed or quality without causing regressions? It's not about volume of AI usage, but effectiveness. A high score could mean the project was delivered faster or with higher quality due to AI assistance (and metrics like cycle time or bug rate support that), and there were no major issues introduced by the AI. A low score might mean either they didn't utilize AI where it could have helped (missed opportunity) or they did, but it caused so many reverts/bugs that it negated any gains.

Use this rubric primarily for coaching, not policing. It's a tool to discuss strengths and areas for growth. For example, if someone scores low in Specification, their coach or lead might work with them to craft better prompts next time. If Verification is weak, maybe the team institutes a checklist for AI changes. Each project or sprint, pick one dimension to focus on improving rather than overwhelming folks with all seven at once. Over time, everyone should level up in all dimensions, but it's a journey.
