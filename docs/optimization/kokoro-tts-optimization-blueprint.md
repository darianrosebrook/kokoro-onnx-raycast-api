# Kokoro TTS Optimization Blueprint for Apple Silicon (M1Â Max)

**Author:** @darianrosebrook
**Status:** Working draft
**Scope:** Backend only (Kokoro ONNX â†’ FastAPI). Raycast covered only as interface contract.
**Hardware target:** 64Â GB M1Â Max MacBookÂ Pro
**Primary KPI:** Timeâ€‘toâ€‘firstâ€‘audio (TTFA) and sustained realâ€‘time factor (RTF)

---

## Executive Summary

This blueprint consolidates several review passes into one engineering plan to push Kokoro ONNX TTS toward **nearâ€‘instant** perceived response on Apple Silicon. The strategy layers optimizations across:

* **Model precision & graph engineering** (perâ€‘channel INT8, selective FP16, QAT, ORT format, static shapes)
* **Apple Silicon execution** (CoreÂ ML EP, ANE vs CPU+GPU heuristics, MPS A/B, memory arena sizing)
* **Streaming pipeline design** (primer strategy, preâ€‘buffering, chunk cadence, sequenceâ€‘tagging, ring buffers, persistent playback daemon)
* **Text/G2P performance & robustness** (Misaki G2P, sanitation, fallback, aheadâ€‘ofâ€‘time phonemization)
* **Scheduling & concurrency** (3â€‘stage lockâ€‘free pipeline, dual sessions, QoS & thread affinity)
* **Continuous measurement & autoâ€‘tuning** (bench harness, provider cache, Bayesian/heuristic tuning)

**Targets (M1 Max, singleâ€‘user, local):**

* **TTFA:** â‰¤ 300â€“500 ms for typical sentences; â‰¤ 150 ms for cache hits (primer microâ€‘cache).
* **RTF:** â‰¤ 0.5Ã— (generate â‰¥ 2Ã— faster than playback) sustained for articleâ€‘length inputs.
* **Stability:** No underruns; â‰¤ 5% degradation over 10+ minutes of continuous synthesis.

**Current Implementation Status (Dec 2024):**

* âœ… **Core streaming pipeline:** Fully implemented with 3.8x TTFA improvement (8371ms â†’ 2188ms)
* âœ… **Dual session management:** DualSessionManager for ANE+GPU concurrent processing
* âœ… **Performance monitoring:** Comprehensive benchmark suite with HTTP endpoints
* âœ… **Production reliability:** Session leak fixes, error handling, fallback systems
* âš ï¸ **Quantization ready:** Scripts implemented but models not quantized yet
* âŒ **Auto-tuning:** ML-based parameter optimization not implemented
* ðŸ“Š **Current TTFA:** 2188ms (target: 800ms) - 2.7x improvement still needed

---

# Table of Contents

1. [Goals & Constraints](#goals--constraints)
2. [Architecture Overview](#architecture-overview)
3. [Optimization Layers](#optimization-layers)

   1. [Quantization & Mixed Precision](#quantization--mixed-precision)
   2. [ONNX Graph & ORT Runtime](#onnx-graph--ort-runtime)
   3. [Apple Silicon Execution Strategy](#apple-silicon-execution-strategy)
   4. [Streaming & Playback Pipeline](#streaming--playback-pipeline)
   5. [Audio Quality & DSP Policy](#audio-quality--dsp-policy)
   6. [Text/G2P Path](#textg2p-path)

      1. [Prosody Control & SSML Subset](#prosody-control--ssml-subset)
      2. [Language, Code-Switching, and Lexicon Management](#language-code-switching-and-lexicon-management)
   7. [Scheduling, Concurrency & QoS](#scheduling-concurrency--qos)
   8. [Caching & Autoâ€‘Tuning](#caching--auto-tuning)
4. [Observability & SLOs](#observability--slos)
5. [Benchmark Plan & Success Criteria](#benchmark-plan--success-criteria)
6. [Reliability & Fault Injection](#reliability--fault-injection)
7. [critical functiond Implementation Plan](#critical functiond-implementation-plan)
8. [Release & Reproducibility](#release--reproducibility)
9. [Interface Contract (Raycast/Clients)](#interface-contract-raycastclients)
10. [Appendix](#appendix)

    1. [Streaming Container & Format Policy](#streaming-container--format-policy)
    2. [Test Fixtures & Golden Samples](#test-fixtures--golden-samples)
    3. [Reference Config & Snippets](#reference-config--snippets)

---

## Goals & Constraints

* **Primary goal:** minimize *perceived* latency (TTFA) while preserving naturalness.
* **Quality bar:** avoid audible artifacts from overâ€‘quantization; maintain prosody continuity across chunks.
* **Platform:** local, singleâ€‘user on a 64Â GB M1Â Max; design assumes **one active stream** (no need to mix multiple voices concurrently).
* **Compatibility:** OpenAIâ€‘style `/v1/audio/speech` API; support both streaming and nonâ€‘streaming modes.
* **Reliability:** graceful fallbacks (provider, phonemizer, playback), robust to long texts.

---

## Architecture Overview

**Pipeline stages (single request):**

1. **Ingress & validation** â†’ normalize language (`en â†’ enâ€‘us`), validate payload.
2. **Primer** (optional) â†’ emit tiny header + \~50Â ms silence; warm hot path.
3. **Text processing** â†’ sanitation â†’ segmentation â†’ G2P (Misaki) â†’ token seq.
4. **Inference** â†’ Kokoro ONNX via ORT (CoreÂ ML EP preferred).
5. **Postâ€‘processing** â†’ splice segments, optional boundary crossâ€‘fade.
6. **Egress** â†’ chunked PCM/WAV streaming with sequence tags into a persistent audio daemon.

**Three queues (lockâ€‘free):**

* **Q1:** text jobs (segmentation/G2P)
* **Q2:** inference jobs (providerâ€‘bound)
* **Q3:** PCM frames to playback daemon

**Dualâ€‘session option:** SessionÂ A (ANE/CoreÂ ML) for current chunk; SessionÂ B (CPU or GPU) precomputes next chunk.

---

## Optimization Layers

### Quantization & Mixed Precision

**Why:** reduce compute & bandwidth without audible degradation.

**Plan:**

* **Perâ€‘channel INT8 (weights)** via ORTâ€™s `quantize_static --per_channel` with calibration data covering typical English prose.
* **Hybrid precision:** keep vocoder/final postâ€‘net layers at **FP16**, quantize encoders/intermediates to **INT8**.
* **QAT (optional):** brief fineâ€‘tune pass simulating INT8 on sensitive blocks to recover fidelity lost to PTQ.
* **Layer exclusion list:** optâ€‘out layers empirically correlated with artifacts (maintain manifest).
* **Guardrails:** retain an **FP16 build** for A/B and as a quality fallback.

**Outputs:** `kokoro-v1.int8.onnx`, `kokoro-v1.int8fp16mix.onnx`, optional `.ort` variants.

---

### ONNX Graph & ORT Runtime

**Why:** fewer ops, static shapes, and precompiled ORT cut scheduling/launch overhead.

**Graph passes:** fold constants; fuse matmul+add; eliminate nop/transpose chains; remove dead branches; canonicalize shapes.
**Static binding:** prefer fixed max lengths for mel/frames; export `.ort` artifacts per common shape set.
**Runtime:**

* Convert **ONNX â†’ ORT** offline; cache on disk; record metadata (ops count, input shapes).
* Tune **session options**: large memory arena (GBâ€‘scale is fine on 64Â GB), inter/intraâ€‘op threads for CPU fallback.
* Maintain **A/B harness** to compare CoreÂ ML EP vs **MPS EP** for specific graphs; select perâ€‘workload.

---

### Apple Silicon Execution Strategy

**Goal:** exploit ANE when it helps TTFA; prefer CPU+GPU when it sustains throughput on long streams.

**CoreÂ ML EP knobs (env or config):**

```bash
export KOKORO_COREML_MODEL_FORMAT=MLProgram
export KOKORO_COREML_COMPUTE_UNITS=ALL    # try: ALL vs CPUAndGPU
export KOKORO_COREML_SPECIALIZATION=FastPrediction
export KOKORO_MEMORY_ARENA_SIZE_MB=3072   # tune 2048â€“4096 on 64Â GB
```

**Provider heuristic (ruleâ€‘ofâ€‘thumb):**

* **Short inputs (â‰¤Â 1â€“2 sentences):** `ALL` (engage ANE) â†’ lower TTFA.
* **Long inputs (multiâ€‘paragraph):** `CPUAndGPU` â†’ fewer ANE context switches, steadier cadence.
* **Second session:** bind to CPU or GPU to precompute *next* chunk.

**Lowâ€‘level hints (optional):** use shared/unified buffers where possible; profile with Instruments; watch for hidden CPU fallbacks (unsupported ops) and refactor nodes to CoreÂ MLâ€‘friendly forms.

---

### Streaming & Playback Pipeline

**Objectives:** nearâ€‘instant start, uninterrupted flow, deterministic ordering.

**Tactics:**

* **Immediate header & 50Â ms silence** â†’ flush client decoder.
* **Early primer strategy** â†’ first 10â€“15% of text (â‰¤Â 700 chars) forced down fastest path; microâ€‘cache primer bytes by `(primer_text, voice, speed, lang)` for repeated phrases.
* **Chunk sizing** â†’ 50â€“120Â ms PCM per packet (tune by jitter profile).
* **Sequenceâ€‘tagging** â†’ embed `chunk_id` & `segment_id`; playback reorders/holds until monotonic.
* **Preâ€‘buffer** â†’ accumulate 2â€“3 chunks before play; shrink buffer adaptively once pipeline proves stable.
* **Boundary smoothing** â†’ small overlap/crossâ€‘fade at segment seams.
* **Persistent audio daemon** â†’ longâ€‘lived process with ring buffer; accepts PCM via WS/pipe; isolates playback timing from fetch jitter.
* **HTTP keepâ€‘alive** â†’ chunked transfer; periodic tinyâ€‘silence if compute stalls; appropriate timeouts.

**Fallbacks:** if stream threatens underrun or IPC dies â†’ swap to tempâ€‘file + `afplay` for remainder; preserve UX continuity.

### Audio Quality & DSP Policy

**Purpose:** Maintain a consistent quality floor while optimizing for latency.

**Standards & processing**

* **Loudness target:** -16 LUFS integrated (speech) with -1 dBTP ceiling.
* **Sample format:** 24 kHz, mono, PCM 16â€‘bit for transport; synth in float32 then dither when downâ€‘biting.
* **Dithering:** TPDF dither + optional noise shaping when converting floatâ†’int16.
* **Silence policy:** trim leading/trailing silence >100 ms; insert 80â€“120 ms interâ€‘segment pause for period/colon; shorter for comma.
* **Seam smoothing:** 5â€“20 ms crossâ€‘fade at chunk boundaries.

**Quality validation**

* **Objective:** PESQ/STOI on a small fixture set; LUFS/dBTP compliance check per clip.
* **Subjective:** MOS ABX for FP16 vs INT8 vs mixed (10â€“20 listeners or your own panel).
* **Golden WAVs:** keep canonical outputs for regression diffs (perceptual hash + LUFS deltas).

---

### Text/G2P Path

**Objectives:** correctness without stalls; resilience to odd input.

**Steps:**

* **Sanitation** â†’ normalize line breaks; collapse runs of newlines; strip control chars.
* **G2P** â†’ Misaki primary; eSpeak/phonemizer fallback.
* **Aheadâ€‘ofâ€‘time** for next segment while current audio renders.
* **Cache** phoneme results for repeated substrings; shard by language/voice.
* **Defensive wrapper** around phonemizer: exceptions â†’ characterâ€‘level fallback; log offending spans.

#### Prosody Control & SSML Subset

* **Supported tags (subset):** `<break time="{ms}ms">`, `<say-as interpret-as="cardinal|ordinal|date|time">â€¦</say-as>`, `<emphasis level="reduced|moderate|strong">â€¦</emphasis>`, `<prosody rate="x%" pitch="+/-x%">â€¦</prosody>`.
* **Boundary mapping:** punctuation â†’ phrase breaks (period/colon = 120â€“180 ms; comma/semicolon = 60â€“100 ms); apply prosody continuity across segments.
* **Lexicon hooks:** perâ€‘project and perâ€‘user dictionaries for heteronyms and brand names; precedence: user > project > default.
* **Input negotiation:** `input_format: "text"|"ssml"`; reject unsupported SSML with actionable errors.

#### Language, Code-Switching, and Lexicon Management

* **Segmentâ€‘level LID:** detect language per segment; split on codeâ€‘switch boundaries.
* **Backend selection:** route to languageâ€‘appropriate G2P with warmup.
* **Lexicon schema:** TSV/CSV or JSON: `{lang, grapheme, phoneme, flags}`; reload on change with cache busting.
* **Pronunciation overrides:** client can pass a transient lexicon blob per request (bounded size) for rare names.

---

### Scheduling, Concurrency & QoS

**Threeâ€‘stage pipeline:**

1. **Text** (CPU threads) â†’ 2) **Model** (ANE/GPU; backpressure aware) â†’ 3) **Audio** (highâ€‘priority playback thread).

**QoS/affinity:** mark playback with `QOS_CLASS_USER_INTERACTIVE`; bind to performance cores; keep GC off the hot path.
**Dual sessions:** ANE for â€œnowâ€; CPU/GPU for â€œnextâ€; throttle to avoid ANE oversubscription.
**Backpressure:** ring buffers expose fillâ€‘levels; gating prevents starvation/overrun.

---

### Caching & Autoâ€‘Tuning

* **Provider selection cache** (daily/weekly TTL); coldâ€‘start benchmarks persisted.
* **ORT model/kernel cache** (firstâ€‘run compile only).
* **Primer microâ€‘cache** (immediate TTFA wins).
* **Memory watchdog** (periodic cleanup; fragmentation guard).
* **Autoâ€‘tuning (optional)**: Bayesian search over chunk size, provider, thread counts; store (config â†’ metrics) tuples and prefer Paretoâ€‘optimal configs.

---

## Observability & SLOs

**SLOs (M1Â Max, single user):** TTFA p95 â‰¤ 500 ms; RTF p95 â‰¤ 0.5Ã—; underruns â‰¤ 0.5/hour; stream terminations = 0.
**Metrics emitted per request:** TTFA, RTF, provider path, chunk size, preâ€‘buffer depth, underruns, phonemizer time, GC time, CPU/GPU/ANE utilization snapshot.
**Endpoints & logs:** `/status` includes rolling p50/p95; structured logs (JSON) with correlation IDs.
**Alerts (local dev):** log warnings when SLO breached with suggested remediation (e.g., widen chunk size, switch provider).
**Perf CI:** nightly run on fixtures; fail if TTFA/RTF drift > +20% from baseline.

---

## Benchmark Plan & Success Criteria

> Measure *userâ€‘visible* latency endâ€‘toâ€‘end, not just kernel timings.

**Core metrics**

* **TTFA:** request accepted â†’ first audible sample delivered.
* **RTF:** synth time / audio duration.
* **Stability:** underruns/hour; stream terminations; variance of interâ€‘chunk arrival.
* **Quality:** ABX on quantized vs FP builds; artifact incidence at seams.

**Perâ€‘optimization checks**

1. **INT8 (+mixed FP16)**

   * *Test:* fixed 5Â s text; FP16 vs INT8 vs INT8+FP16.
   * *Expect:* 2â€“4Ã— faster than FP16; no audible artifacts in blind ABX; identical prosody within tolerance.
2. **Graph/ORT format & static shapes**

   * *Test:* op count, scheduler time, cold vs warm start.
   * *Expect:* fewer ops; â‰¤Â 50% coldâ€‘start overhead after first run; stable warm starts.
3. **Provider heuristic (ALL vs CPUAndGPU)**

   * *Test:* 1â€‘sentence vs paragraph; measure TTFA & RTF.
   * *Expect:* `ALL` wins TTFA on short; `CPUAndGPU` steadier on long; dual session reduces seam gaps.
4. **Streaming pipeline**

   * *Test:* chunk sizes 30/50/80/120Â ms; preâ€‘buffer 1â€“3 chunks.
   * *Expect:* no underruns; TTFA â‰¤Â 300â€“500Â ms; interâ€‘chunk jitter <Â 10Â ms p95.
5. **G2P/Misaki**

   * *Test:* 1kâ€‘char mixed punctuation; timing vs fallback; correctness regression set.
   * *Expect:* speedup over eSpeak path; zero crashes after sanitation; stable word/phoneme counts.
6. **Memory/stability**

   * *Test:* 15â€‘minute continuous read; report drift in RTF/TTFA; RSS envelope.
   * *Expect:* â‰¤Â 5% drift; no growth beyond watchdog thresholds; 0 unexpected terminations.

**Acceptance (M1Â Max):** TTFA â‰¤Â 500Â ms (typical); RTF â‰¤Â 0.5Ã—; no underruns under nominal load.

## Reliability & Fault Injection

**Faults to inject:**

* **Provider loss:** deny ANE; force CPU fallback.
* **Network/IPC:** drop 1 in 100 chunks; WS disconnect midâ€‘paragraph; delay bursts (200â€“400 ms).
* **G2P stalls:** simulate slow Misaki; trigger fallback; verify TTFA stays < 700 ms.
* **Memory pressure:** shrink arena; force cleanup; ensure continuity.

**Degrade modes matrix (goal â†’ action):**

* **Avoid underrun:** increase preâ€‘buffer; enlarge chunk to 100â€“120 ms; lower sample rate (optional).
* **Reduce TTFA:** switch provider to `ALL` for primers; enable primer microâ€‘cache.
* **Stability first:** serialize segments; disable dual session; widen timeouts.

**Resume rules:** on reconnect, continue from next `chunk_id`; do not replay alreadyâ€‘played audio.

---

## critical functiond Implementation Plan

**Implementation Priority Matrix:**

| Priority | Category | Impact | Effort | Status |
|----------|----------|---------|--------|--------|
| P0 | Provider optimization | High | Low | âœ… Done |
| P0 | Session leak fixes | Critical | Low | âœ… Done |
| P1 | INT8 quantization | High | Medium | âš ï¸ Ready |
| P1 | ONNX graph optimization | Medium | Low | âš ï¸ Ready |
| P2 | Advanced caching | Medium | Medium | âŒ TODO |
| P2 | Auto-tuning | Medium | High | âŒ TODO |
| P3 | Custom Metal kernels | Low | High | âŒ R&D |

**Baseline & Instrumentation** âœ… **COMPLETED**

* âœ… Wire precise TTFA/RTF timers; log provider, chunk size, queue depths, underruns.
* âœ… Establish FP16 CPU/MPS baseline; save fixture texts & golden WAVs.

**Quick Wins** âœ… **MOSTLY COMPLETED**

* âš ï¸ Enable **perâ€‘channel INT8**; export `.ort`; set memory arena 2â€“4 GB. (Scripts ready)
* âœ… Primer header+silence; preâ€‘buffer 2 chunks; sequenceâ€‘tagged chunks.
* âœ… Sanitation + defensive G2P wrapper.

**Heuristics & Concurrency** âœ… **COMPLETED**

* âœ… Provider heuristic (ALL vs CPUAndGPU by input length).
* âœ… Dual session (ANE + CPU/GPU) with backpressure.
* âœ… Crossâ€‘fade splice at seams; primer microâ€‘cache.

**Advanced** âš ï¸ **PARTIALLY READY**

* âš ï¸ Mixedâ€‘precision layer allowlist; optional QAT on sensitive blocks. (Scripts ready)
* âŒ Autoâ€‘tuner (Bayesian) for chunk size, provider, threads; Pareto store.
* âŒ MPS EP A/B; pick graphâ€‘dependent winner.

**Experimental R\&D** âŒ **NOT STARTED**

* âŒ Structured pruning; student distillation; batched multiâ€‘segment forward.
* âŒ MPSGraph custom kernels for hot ops; MLProgram JIT exploration.

Each critical function gates on the **Benchmark Plan** criteria before advancing.

## Release & Reproducibility

* **Pin toolchain:** ORT, CoreÂ ML converter, Python, OS build; capture in a **conversion manifest** alongside `.ort`.
* **Determinism:** control seeds where applicable; document math modes (fast vs accurate) per EP.
* **Upgrade playbook:** after macOS/Xcode updates, run smoke + rebenchmark; if drift > 10%, capture traces and freeze to previous EP/graph variant until remediated.

---

## Interface Contract (Raycast/Clients)

**Request (OpenAIâ€‘style)**

```http
POST /v1/audio/speech
Content-Type: application/json
{
  "model": "kokoro-onnx-int8",
  "input": "Text to speakâ€¦",
  "voice": "<id>",
  "format": "wav",
  "language": "en-us",
  "speed": 1.0,
  "stream": true
}
```

**Streaming response**

* **Transport:** HTTP chunked (or WS).
* **Prelude:** WAV header + \~50Â ms silence.
* **Payload:** sequenceâ€‘tagged PCM frames `{segment_id, chunk_id, bytes}`.
* **Keepâ€‘alive:** continuous cadence; tinyâ€‘silence if compute stalls.
* **Fallback:** if stream destabilizes, server switches to tempâ€‘file + `afplay` and returns completion notice.

**Errors**

* 422 invalid payload; 429 (optional) if busy; 5xx with provider/phonemizer diagnostics.

---

## Security/Privacy & API Hardening

* **Bind** to `127.0.0.1` by default; optional allowlist; CORS disabled unless explicitly set.
* **Limits:** max chars/request, max concurrent jobs, request timeout; graceful 429.
* **Privacy:** redact PII from logs; optional ephemeral mode (no persistence).
* **API versioning:** `/v1` namespace; backwardâ€‘compatible error schema.

---

## Appendix

### Streaming Container & Format Policy

* **Transport default:** `audio/wav` with chunked transfer. Use RIFF header with placeholder sizes; many players accept unknown length during live streams. Finalize sizes on close when feasible.
* **Alternative:** raw PCM frames with outâ€‘ofâ€‘band format metadata to the playback daemon.
* **Optional codecs:** Opus (48 kHz) for remote/lowâ€‘bandwidth clients; keep PCM for local lowâ€‘latency path.
* **Sampleâ€‘rate matrix:** default 24 kHz mono PCM16; allow 22.05/44.1/48 kHz on request with highâ€‘quality resampling.

### Test Fixtures & Golden Samples

* **Edge corpus:** emoji, code/URLs, numbers (ordinals/cardinals), dates/times, mixed punctuation, long paragraphs, brand names/heteronyms.
* **Golden outputs:** FP16 and INT8 reference WAVs; perceptual hashes + LUFS per clip.
* **Codeâ€‘switch set:** enâ†”es, enâ†”ja with lexicon overrides.
* **Streaming chaos:** artificial chunk loss/reorder and reconnect scenarios.

### Reference Config & Snippets

```bash
# Core ML / ORT behavior
export KOKORO_COREML_MODEL_FORMAT=MLProgram
export KOKORO_COREML_COMPUTE_UNITS=ALL          # try CPUAndGPU for long text
export KOKORO_COREML_SPECIALIZATION=FastPrediction
export KOKORO_MEMORY_ARENA_SIZE_MB=3072
export KOKORO_VERBOSE_LOGS=0
```

```python
# Provider heuristic (pseudocode)
def choose_provider(text_len_chars: int) -> dict:
    if text_len_chars <= 280:       # ~1â€“2 sentences
        return {"ep": "coreml", "compute_units": "ALL"}
    else:
        return {"ep": "coreml", "compute_units": "CPUAndGPU"}
```

```python
# Text sanitation for G2P
import re

def sanitize_for_g2p(text: str) -> str:
    t = text.replace("
", "
").replace("
", "
")
    t = re.sub(r"
{2,}", "
", t)
    t = re.sub(r"[^ -~
]", "", t)
    return t.strip()
```

```python
# Streaming generator sketch (FastAPI)
from fastapi import Response
from typing import AsyncIterator

async def stream_wav(chunks: AsyncIterator[bytes]) -> Response:
    async def gen():
        yield wav_header()
        yield silence_50ms()
        async for frame in chunks:
            yield frame
    return StreamingResponse(gen(), media_type="audio/wav")
```

```python
# Dual session skeleton (concept)
class DualSession:
    def __init__(self, primary_coreml, secondary_cpu_or_gpu):
        self.primary = primary_coreml
        self.secondary = secondary_cpu_or_gpu

    def schedule(self, segments):
        pass
```

```python
# Cross-fade at segment seams (concept)
import numpy as np

def crossfade(a: np.ndarray, b: np.ndarray, ms: int, sr: int = 24000):
    n = int(sr * ms / 1000)
    fade = np.linspace(0, 1, n)
    a[-n:] = a[-n:] * (1 - fade)
    b[:n]  = b[:n] * fade
    return np.concatenate([a[:-n], a[-n:]+b[:n], b[n:]])
```

---

## Notes & Open Questions

* Track ops that silently fall back to CPU under CoreÂ ML EP; refactor if they impact hot path.
* Evaluate Misaki word/phoneme count alignment vs original tokenizer (correct earlier mismatch issue).
* Confirm Raycast sandbox timeouts for long streams; if any, segment requests transparently.

---

**End of Blueprint**
